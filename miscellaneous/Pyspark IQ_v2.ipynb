{"cells":[{"cell_type":"markdown","source":["# 01. How do you handle duplicate rows in a PySpark DataFrame?"],"metadata":{"nteract":{"transient":{"deleting":false}},"jp-MarkdownHeadingCollapsed":true},"id":"7633f39c-148b-496e-ac52-0cb092b51929"},{"cell_type":"code","source":["# Welcome to your new notebook\n","# Type here in the cell editor to add code!\n","# Sample Data\n","data = [\n","    (1, \"Alice\", 25),\n","    (2, \"Alice\", 30),\n","    (1, \"Alice\", 25),  # Duplicate row\n","    (3, \"Charlie\", 35)\n","]\n","\n","columns = [\"ID\", \"Name\", \"Age\"]\n","df = spark.createDataFrame(data=data,schema=columns)\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T19:55:54.1684656Z","session_start_time":null,"execution_start_time":"2025-02-06T19:55:54.308565Z","execution_finish_time":"2025-02-06T19:55:55.1020414Z","parent_msg_id":"3da18f88-8f07-429d-8ddc-09cb72dbd34f"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"c1759277-85aa-4c03-9c99-6206e771eca9","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, c1759277-85aa-4c03-9c99-6206e771eca9)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"aa5fcfa0-2067-4c08-bf55-7752a20a181b"},{"cell_type":"code","source":["df_no_duplicates = df.dropDuplicates()\n","display(df_no_duplicates)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T19:56:00.5706264Z","session_start_time":null,"execution_start_time":"2025-02-06T19:56:00.7410434Z","execution_finish_time":"2025-02-06T19:56:01.5508825Z","parent_msg_id":"d01530d2-e599-43ad-a5ec-6e939c2942f5"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 12, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"768aa14f-e6f6-477b-bbce-1045350bb0eb","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 768aa14f-e6f6-477b-bbce-1045350bb0eb)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"5184d2b1-1c5f-49bc-b7ab-b5b6374a7a9c"},{"cell_type":"code","source":["# Remove Duplicates Based on \"Name\" Column\n","df_no_duplicates = df.dropDuplicates([\"Name\"])\n","\n","df_no_duplicates.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T19:56:08.6258284Z","session_start_time":null,"execution_start_time":"2025-02-06T19:56:08.8295289Z","execution_finish_time":"2025-02-06T19:56:09.9744418Z","parent_msg_id":"7cc36e1d-b5ac-4f68-96a8-bb2121c48f01"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+-------+---+\n| ID|   Name|Age|\n+---+-------+---+\n|  1|  Alice| 25|\n|  3|Charlie| 35|\n+---+-------+---+\n\n"]}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed48a93e-5601-4f85-b53d-061c4e94e812"},{"cell_type":"code","source":["# Identify & Handle Duplicates Using Window Functions\n","from pyspark.sql.window import Window\n","from pyspark.sql.functions import row_number\n","window_spec = Window.partitionBy(\"ID\", \"Name\", \"Age\").orderBy(\"ID\")\n","\n","df_with_rownum = df.withColumn(\"row_number\",row_number().over(window_spec))\n","df_with_rownum.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T20:02:56.3682137Z","session_start_time":null,"execution_start_time":"2025-02-06T20:02:56.5359635Z","execution_finish_time":"2025-02-06T20:02:57.3035315Z","parent_msg_id":"d201d396-dacf-4b1b-9ab9-57fdd20ca76b"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 20, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+-------+---+----------+\n| ID|   Name|Age|row_number|\n+---+-------+---+----------+\n|  1|  Alice| 25|         1|\n|  1|  Alice| 25|         2|\n|  2|  Alice| 30|         1|\n|  3|Charlie| 35|         1|\n+---+-------+---+----------+\n\n"]}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae4b66de-ba9e-4736-a267-0385d230f842"},{"cell_type":"code","source":["df_deduplicated = df_with_rownum.filter(\"row_number = 1\").drop(\"row_number\")\n","\n","df_deduplicated.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":21,"statement_ids":[21],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T20:03:01.4015729Z","session_start_time":null,"execution_start_time":"2025-02-06T20:03:01.6670338Z","execution_finish_time":"2025-02-06T20:03:02.6147637Z","parent_msg_id":"a0ba07b7-8575-4833-8195-572e3fdf5ec0"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 21, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+-------+---+\n| ID|   Name|Age|\n+---+-------+---+\n|  1|  Alice| 25|\n|  2|  Alice| 30|\n|  3|Charlie| 35|\n+---+-------+---+\n\n"]}],"execution_count":19,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be5e6dca-c2bc-412e-ab8a-133480ca6686"},{"cell_type":"code","source":["from pyspark.sql.functions import count\n","\n","df_counts = df.groupBy(\"ID\", \"Name\", \"Age\").count().orderBy(\"count\", ascending=False)\n","\n","df_counts.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"a8a75a02-ed97-41cd-9029-ccb4afdbfd67","normalized_state":"finished","queued_time":"2025-02-06T20:03:55.9028548Z","session_start_time":null,"execution_start_time":"2025-02-06T20:03:56.067409Z","execution_finish_time":"2025-02-06T20:03:56.8632749Z","parent_msg_id":"45e091f5-a37d-4f2a-95c1-572f67c9b79b"},"text/plain":"StatementMeta(, a8a75a02-ed97-41cd-9029-ccb4afdbfd67, 22, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+---+-------+---+-----+\n| ID|   Name|Age|count|\n+---+-------+---+-----+\n|  1|  Alice| 25|    2|\n|  2|  Alice| 30|    1|\n|  3|Charlie| 35|    1|\n+---+-------+---+-----+\n\n"]}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34972705-2253-456c-b184-a22ca47f568f"},{"cell_type":"markdown","source":["- âœ… Remove all duplicates? â†’ dropDuplicates()\n","- âœ… Remove based on specific columns? â†’ dropDuplicates([\"col1\", \"col2\"])\n","- âœ… Identify and analyze duplicates? â†’ row_number() with Window\n","- âœ… Count duplicates? â†’ groupBy().count()"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f59f1f05-7b57-4f2a-b18e-8921c3edf307"},{"cell_type":"markdown","source":["# 02. What makes Apache Spark unique compared to other big data frameworks?"],"metadata":{"nteract":{"transient":{"deleting":false}},"jp-MarkdownHeadingCollapsed":true},"id":"98603d63-da74-431c-97ad-3a62d1aac9af"},{"cell_type":"markdown","source":["- Apache Spark stands out among big data frameworks due to its speed, ease of use, and versatility.\n","- Hereâ€™s what makes Spark unique compared to other frameworks like Hadoop MapReduce, Flink, and Dask:\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed2a95ca-23c6-4fcd-a7ab-6ae15750e600"},{"cell_type":"markdown","source":["**01. Speed â€“ In-Memory Processing**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Unlike Hadoop MapReduce, which writes intermediate data to disk, Spark processes data in memory (RAM), making it 100x faster for some workloads.\n","- Uses Resilient Distributed Datasets (RDDs) to minimize disk I/O."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"62a57204-faa3-4c5e-adf1-d8535e2758a6"},{"cell_type":"markdown","source":["**02. Unified Data Processing Engine**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Spark is a multi-purpose engine supporting batch, streaming, machine learning, and SQL-based analytics in a single framework.\n","- Other frameworks like Hadoop focus mainly on batch processing, and Flink specializes in real-time streaming.\n","\n","**âœ… Components of Spark:**\n","\n","**Component\tPurpose**\n","- Spark Core\t> Handles distributed computing with RDDs\n","- Spark SQL\t> Query data using SQL & DataFrames\n","- Spark Streaming\t> Real-time data processing (Kafka, Flume)\n","- MLlib\t> Machine learning algorithms\n","- GraphX\t> Graph analytics (social networks, recommendation systems)\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1eafd512-61c7-4a76-aa73-b900c2398bc4"},{"cell_type":"markdown","source":["**03. Supports Multiple Languages**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Spark supports **Python (PySpark), Scala, Java, and R** for big data development.\n","- Hadoop MapReduce mainly uses Java, which can be complex for data analysts and scientists."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc7e5af5-a988-4432-a7e5-abadc49b5cfe"},{"cell_type":"markdown","source":["**04. Built-In Optimizations with Catalyst & Tungsten**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Spark uses the Catalyst Optimizer (for query optimization) and Tungsten Engine (for better memory & CPU management).\n","- Other frameworks donâ€™t have such advanced query optimizations.\n","\n","**âœ… Example:**\n","- When you run **df.groupBy(\"column\").count()**, Spark automatically optimizes execution plans to minimize computation."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"549324fa-e621-4482-9a13-b11800402643"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"CatalystExample\").getOrCreate()\n","\n","# Creating DataFrame\n","data = [(1, \"Alice\", 30), (2, \"Bob\", 25), (3, \"Charlie\", 35)]\n","df = spark.createDataFrame(data, [\"ID\", \"Name\", \"Age\"])\n","\n","# Running an optimized query\n","df_filtered = df.filter(\"Age > 30\").select(\"Name\")\n","\n","df_filtered.explain(True)  # Show Catalyst Execution Plan\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"a834627b-8505-4665-aecf-c03ee4e1417b","normalized_state":"finished","queued_time":"2025-02-06T21:11:22.0555313Z","session_start_time":null,"execution_start_time":"2025-02-06T21:11:22.2309975Z","execution_finish_time":"2025-02-06T21:11:23.7444289Z","parent_msg_id":"8d86f019-a2cc-4346-a09e-edb6ae6c1689"},"text/plain":"StatementMeta(, a834627b-8505-4665-aecf-c03ee4e1417b, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["== Parsed Logical Plan ==\n'Project ['Name]\n+- Filter (Age#696L > cast(30 as bigint))\n   +- LogicalRDD [ID#694L, Name#695, Age#696L], false\n\n== Analyzed Logical Plan ==\nName: string\nProject [Name#695]\n+- Filter (Age#696L > cast(30 as bigint))\n   +- LogicalRDD [ID#694L, Name#695, Age#696L], false\n\n== Optimized Logical Plan ==\nProject [Name#695]\n+- Filter (isnotnull(Age#696L) AND (Age#696L > 30))\n   +- LogicalRDD [ID#694L, Name#695, Age#696L], false\n\n== Physical Plan ==\n*(1) Project [Name#695]\n+- *(1) Filter (isnotnull(Age#696L) AND (Age#696L > 30))\n   +- *(1) Scan ExistingRDD[ID#694L,Name#695,Age#696L]\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7f3404b-4083-49f8-a595-6e4216892e13"},{"cell_type":"code","source":["df_filtered.explain(\"cost\")  # Shows Tungsten-optimized execution plan"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"a834627b-8505-4665-aecf-c03ee4e1417b","normalized_state":"finished","queued_time":"2025-02-06T21:11:39.0746555Z","session_start_time":null,"execution_start_time":"2025-02-06T21:11:39.2240616Z","execution_finish_time":"2025-02-06T21:11:39.4558268Z","parent_msg_id":"27b1eb06-653b-4759-b3dd-cc1be1c2333b"},"text/plain":"StatementMeta(, a834627b-8505-4665-aecf-c03ee4e1417b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["== Optimized Logical Plan ==\nProject [Name#695], Statistics(sizeInBytes=5.1 EiB)\n+- Filter (isnotnull(Age#696L) AND (Age#696L > 30)), Statistics(sizeInBytes=8.0 EiB)\n   +- LogicalRDD [ID#694L, Name#695, Age#696L], false, Statistics(sizeInBytes=8.0 EiB)\n\n== Physical Plan ==\n*(1) Project [Name#695]\n+- *(1) Filter (isnotnull(Age#696L) AND (Age#696L > 30))\n   +- *(1) Scan ExistingRDD[ID#694L,Name#695,Age#696L]\n\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ece4aac6-2e09-4cf0-82f3-e03c986b6742"},{"cell_type":"markdown","source":["**05.Real-Time & Batch Processing Together**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Supports both batch processing (like Hadoop) and real-time streaming (like Flink).\n","- Spark Structured Streaming allows processing continuous data streams with micro-batch techniques.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"2a51fcd1-68c4-4269-aebd-033b20566274"},{"cell_type":"code","source":["#df = spark.readStream.format(\"kafka\").option(\"subscribe\", \"topic1\").load()\n","#df.writeStream.outputMode(\"append\").format(\"console\").start()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"7f872deb-f7be-464e-9563-ad2b76f1c4ca","normalized_state":"finished","queued_time":"2025-02-06T22:16:12.4285688Z","session_start_time":null,"execution_start_time":"2025-02-06T22:16:12.6086023Z","execution_finish_time":"2025-02-06T22:16:12.8666716Z","parent_msg_id":"bd06110b-d1ce-46e8-8d73-2f83178cbeed"},"text/plain":"StatementMeta(, 7f872deb-f7be-464e-9563-ad2b76f1c4ca, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"919c4383-f15b-46c6-a123-f77270b75b45"},{"cell_type":"markdown","source":["**6. Works with Any Storage System**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Spark can read/write data from HDFS, Amazon S3, Azure Blob, Google Cloud Storage, and even databases.\n","- Hadoop primarily relies on HDFS."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"04b24f6a-28ab-44dd-b241-f1e6cba74d62"},{"cell_type":"code","source":["#df = spark.read.csv(\"wasbs://container@account.blob.core.windows.net/data.csv\")\n","#df.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"7f872deb-f7be-464e-9563-ad2b76f1c4ca","normalized_state":"finished","queued_time":"2025-02-06T22:19:27.301856Z","session_start_time":null,"execution_start_time":"2025-02-06T22:19:27.511045Z","execution_finish_time":"2025-02-06T22:19:27.7550761Z","parent_msg_id":"44a7b229-ada3-4691-a9ce-dee8f1181073"},"text/plain":"StatementMeta(, 7f872deb-f7be-464e-9563-ad2b76f1c4ca, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"490b393d-d039-49bd-9f0b-283debaa888c"},{"cell_type":"markdown","source":["**7. Scalability & Fault Tolerance**\n","\n","ðŸ”¥ Why itâ€™s unique?\n","\n","- Spark runs on-premises or in the cloud (AWS EMR, Azure Databricks, GCP DataProc).\n","- Fault tolerance is built-in using RDD lineage (automatically recomputes lost data).\n","- âœ… Example: If a node in the cluster fails, Spark recomputes lost partitions without restarting the job."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"aed1363b-3cd4-48e8-89d9-67ab30e681de"},{"cell_type":"markdown","source":["**Summary: Why Choose Spark?**\n","\n","| Feature |\tApache Spark |\tHadoop MapReduce |\tFlink |\n","| --- | --- | --- | --- |\n","| Processing Type\t| Batch + Streaming\t| Batch Only\t| Streaming First\n","| Speed\t| Fast (In-memory)\t| Slow (Disk-based)\t| Real-time| \n","| Ease of Use\t| Python, Scala, SQL\t| Java-heavy\t| Java, Scala|\n","| Optimizations\t| Catalyst, Tungsten| \tNo major optimizer\t| Stream optimizations|\n","| Machine Learning\t| Yes (MLlib)\t|No\t|No|\n","| Fault Tolerance\t| Yes (RDD lineage)\t| Yes (HDFS)\t|Yes|\n","| Best For\t| General Big Data\t| Large Batch Jobs\t| Real-time Analytics"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1b90f707-7590-4e9c-af0b-89235a03a920"},{"cell_type":"markdown","source":["**Conclusion:**\n","- If you need fast, multi-purpose data processing, choose Spark.\n","- If you have huge batch jobs and donâ€™t need real-time, Hadoop is fine.\n","- If you want ultra-low latency streaming, Apache Flink may be better."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"750642b1-3a11-462e-806f-24431e7a33da"},{"cell_type":"markdown","source":["# 03. Catalyst Optimizer & Tungsten Engine in Apache Spark"],"metadata":{"nteract":{"transient":{"deleting":false}},"jp-MarkdownHeadingCollapsed":true},"id":"00facb42-c478-41ba-91d0-3819da677b21"},{"cell_type":"markdown","source":["\n","\n","- Catalyst is Sparkâ€™s cost-based optimizer used in Spark SQL and DataFrames. It automatically optimizes query execution plans.\n","- Catalyst follows four phases to optimize queries:\n","\n","1. Analysis Phase\n","\n","- Checks syntax and resolves column names.\n","- Uses metadata from Hive Metastore, JDBC, or Data Catalog.\n","\n","2. Logical Optimization Phase\n","\n","- Rewrites inefficient queries (e.g., predicate pushdown, constant folding).\n","- Converts filters before joins for better performance.\n","\n","3. Physical Planning Phase\n","\n","- Generates multiple execution plans and selects the best one based on cost estimation.\n","\n","4. Code Generation Phase (Tungsten Integration)\n","\n","- Uses whole-stage code generation (WSCG) to convert Spark queries into efficient bytecode.\n","\n","**Tungsten**\n","\n","- Tungsten improves Sparkâ€™s performance by optimizing memory usage, CPU efficiency, and serialization.\n","- Key Features of Tungsten\n","\n","**01. Binary Processing**\n","\n","- Uses off-heap memory to reduce Java garbage collection (GC) overhead.\n","- Stores data in binary format instead of JVM objects.\n","\n","**02. Whole-Stage Code Generation (WSCG)**\n","\n","- Converts high-level Spark operations into efficient Java bytecode.\n","- Removes JVM interpretation overhead, making it 10x faster than standard execution.\n","\n","**03. Cache-Aware Computation**\n","\n","- Uses CPU registers and vectorized processing for faster computation.\n","- Efficient CPU cache usage, reducing memory access time.\n","\n","**04. Efficient Memory Management**\n","\n","- Uses row-based & columnar storage for optimized execution.\n","- Avoids unnecessary object creation, reducing GC pressure.\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a790809b-652b-4cbb-b713-87735b84e79a"},{"cell_type":"markdown","source":["# 04. Explain different profilers in PySpark and their use cases."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"0c5422d8-ca3a-48b1-a2f5-016c58b96a5c"},{"cell_type":"markdown","source":["| Profiler Type\t| Performance Overhead\t| Use Case |\n","| --- | --- | --- |\n","| BasicProfiler |\tLow\t| Quick profiling for small jobs |\n","| Custom Profiler\t| Medium\t| Custom profiling logic|\n","| CProfileProfiler |\tHigh\t|Detailed function call profiling |"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6f011e7c-7d3f-4e0b-bc30-dd8052d1b340"},{"cell_type":"markdown","source":["**Key Takeaways**\n","- Use BasicProfiler for simple debugging.\n","- Use CProfileProfiler for detailed function-level performance analysis.\n","- Implement a Custom Profiler when you need specific profiling metrics beyond the built-in ones."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"166a6bc0-18c9-4f2f-821c-143799a03085"},{"cell_type":"markdown","source":["# 05. Write a PySpark program to check if a keyword exists in a large text file."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1021ddaa-c486-43e8-9026-b529dec66d8b"},{"cell_type":"code","source":["# download file and upload into fabric\n","# https://raw.githubusercontent.com/rritec/Azure-Cloud-Data-Engineering/refs/heads/main/Lab%20Data/sample.txt"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"8ffb8c26-95b3-42cb-b9db-d3d9d1de0846","normalized_state":"finished","queued_time":"2025-02-07T17:36:08.9351807Z","session_start_time":null,"execution_start_time":"2025-02-07T17:36:09.0893478Z","execution_finish_time":"2025-02-07T17:36:09.3484765Z","parent_msg_id":"f17fb992-08d9-4496-8494-8029f9b8ab1c"},"text/plain":"StatementMeta(, 8ffb8c26-95b3-42cb-b9db-d3d9d1de0846, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e3678994-cbae-49a4-96c2-694fcbefdf15"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col\n","\n","# Initialize Spark Session (Fabric manages it automatically)\n","#spark = SparkSession.builder.appName(\"KeywordSearchFabric\").getOrCreate()\n","\n","# Define file path in OneLake or ADLS\n","file_path = \"abfss://rrws1@onelake.dfs.fabric.microsoft.com/rrLakeHouse.Lakehouse/Files/data/sample.txt\"  # Update path\n","\n","# Define the keyword to search\n","keyword = \"hello\"\n","\n","# Load the text file as a DataFrame\n","df = spark.read.text(file_path)\n","\n","# Perform case-insensitive keyword search using regex (rlike)\n","matching_rows = df.filter(col(\"value\").rlike(f\"(?i){keyword}\"))\n","\n","# Stop execution early if at least one match is found\n","if matching_rows.limit(1).count() > 0:\n","    print(f\"âœ… Keyword '{keyword}' found in the file.\")\n","else:\n","    print(f\"âŒ Keyword '{keyword}' NOT found in the file.\")\n","\n","# Stop Spark Session (Fabric handles cleanup automatically)\n","#spark.stop()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"8ffb8c26-95b3-42cb-b9db-d3d9d1de0846","normalized_state":"finished","queued_time":"2025-02-07T17:39:44.9422743Z","session_start_time":null,"execution_start_time":"2025-02-07T17:39:45.1012409Z","execution_finish_time":"2025-02-07T17:39:49.8676894Z","parent_msg_id":"290553fe-ab89-40ac-91c5-4e387c189a37"},"text/plain":"StatementMeta(, 8ffb8c26-95b3-42cb-b9db-d3d9d1de0846, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["âœ… Keyword 'hello' found in the file.\n"]}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"873bbdbc-8f6c-4021-92cd-54123c8c1086"},{"cell_type":"code","source":["# For case-insensitive whole-word match:\n","# df.filter(col(\"value\").rlike(f\"(?i)\\\\b{keyword}\\\\b\"))\n","\n","# For multiple keyword search:\n","# df.filter(col(\"value\").rlike(f\"(?i)\\\\b(spark|big data)\\\\b\"))\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"8ffb8c26-95b3-42cb-b9db-d3d9d1de0846","normalized_state":"finished","queued_time":"2025-02-07T17:46:52.7170329Z","session_start_time":null,"execution_start_time":"2025-02-07T17:46:52.9126718Z","execution_finish_time":"2025-02-07T17:46:53.1682996Z","parent_msg_id":"584eedde-2681-478c-9a26-1fc238831f6b"},"text/plain":"StatementMeta(, 8ffb8c26-95b3-42cb-b9db-d3d9d1de0846, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c18973ad-ac04-4c39-a131-7ed66bdee032"},{"cell_type":"markdown","source":["# 06. What is Executor Memory in PySpark, and how is it allocated?"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"b9fa98ba-7f64-48e1-9dfb-7255baba3a1c"},{"cell_type":"code","source":["# get all conf parameters\n","for key, value in spark.sparkContext.getConf().getAll():\n","    if key == \"spark.executor.memory\":\n","        print(f\"{key} = {value}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"8ffb8c26-95b3-42cb-b9db-d3d9d1de0846","normalized_state":"finished","queued_time":"2025-02-07T17:55:27.9577292Z","session_start_time":null,"execution_start_time":"2025-02-07T17:55:28.1717977Z","execution_finish_time":"2025-02-07T17:55:28.4313391Z","parent_msg_id":"47ae5312-a195-4316-810d-90c1edaeed07"},"text/plain":"StatementMeta(, 8ffb8c26-95b3-42cb-b9db-d3d9d1de0846, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["spark.executor.memory = 56g\n"]}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c190e59b-ff55-4b7b-87d6-62acee57bd25"},{"cell_type":"code","source":["# Define the keys we want to check\n","keys_to_check = {\"spark.executor.memory\", \"spark.executor.memoryOverhead\"}\n","\n","# Get all Spark configurations and filter the required keys\n","config_values = {key: value for key, value in spark.sparkContext.getConf().getAll() if key in keys_to_check}\n","\n","# Print the values\n","for key, value in config_values.items():\n","    print(f\"{key} = {value}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"8ffb8c26-95b3-42cb-b9db-d3d9d1de0846","normalized_state":"finished","queued_time":"2025-02-07T17:58:02.4832766Z","session_start_time":null,"execution_start_time":"2025-02-07T17:58:02.6982397Z","execution_finish_time":"2025-02-07T17:58:03.5835577Z","parent_msg_id":"a523b607-55ef-4936-8cc6-174f771632b9"},"text/plain":"StatementMeta(, 8ffb8c26-95b3-42cb-b9db-d3d9d1de0846, 16, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["spark.executor.memory = 56g\nspark.executor.memoryOverhead = 384\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a45cc770-fb97-4b4b-9b4a-df075bb5640f"},{"cell_type":"markdown","source":["- Executor Memory in PySpark refers to the amount of memory allocated to each Spark executor in a cluster. Executors are responsible for executing tasks and storing data in memory.\n","\n","**How Executor Memory is Allocated**\n","\n","- Executor memory is divided into three main categories:\n","\n","**1. Execution Memory**\n","\n","- Used for runtime computations, such as:\n","1. Shuffling\n","2. Joins\n","3. Sorting\n","4. Aggregation\n","\n","- If memory is not used for execution, it can be dynamically borrowed by storage.\n","\n","**2. Storage Memory**\n","\n","- Used for caching datasets (RDD cache, DataFrame cache).\n","- If the cached data is not used, it can be borrowed by execution memory.\n","\n","**3. Overhead Memory**\n","\n","- Memory reserved for Spark's internal processes, including:\n","\n","1. Broadcast variables\n","2. Task metadata\n","3. Garbage collection (GC)\n","\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70c1e0bf-80b9-4d05-97e5-a2076d0806a1"},{"cell_type":"markdown","source":["# 07. How do you minimize data transfers in PySpark applications?"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"441bd21c-8a5c-483c-a19a-4ab469868b18"},{"cell_type":"markdown","source":["| Technique\t| Why It Works? |\n","| --- | --- |\n","| Broadcast small datasets |\tAvoids shuffling in joins.|\n","| Reduce shuffle operations |\tPrevents unnecessary data movement.|\n","| Cache/Persist reused data\t| Avoids recomputation and reloading.|\n","| Use columnar formats (Parquet)\t| Reduces data transfer and I/O costs.\n","Push filters early (Predicate Pushdown)\t| Reduces data read before processing.\n","Use mapPartitions() instead of map()\t| Processes larger chunks, reducing network traffic.\n","Optimize joins\t| Align partitions and broadcast small datasets.\n","Avoid .collect() and .toPandas()\t| Prevents memory overflow at the driver."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5699afd6-81c2-4ca0-a48b-33d291e41405"},{"cell_type":"markdown","source":["# 08. What is Piping in PySpark, and how does it work?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ad8ce028-5baa-467d-8313-fa338d9ae8ba"},{"cell_type":"code","source":["rdd = spark.sparkContext.parallelize([\"Hello World\", \"PySpark is awesome\", \"Piping in Spark\"])\n","rdd.collect()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"14995f92-c7e5-424b-b874-0ae4d6ab2197","normalized_state":"finished","queued_time":"2025-02-07T19:43:31.6084207Z","session_start_time":null,"execution_start_time":"2025-02-07T19:43:31.7593668Z","execution_finish_time":"2025-02-07T19:43:32.011834Z","parent_msg_id":"7d93425a-77c5-48d7-a56e-029a51a7bbed"},"text/plain":"StatementMeta(, 14995f92-c7e5-424b-b874-0ae4d6ab2197, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"['Hello World', 'PySpark is awesome', 'Piping in Spark']"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"09a756d0-b52c-40e9-a458-0c84b8505048"},{"cell_type":"code","source":["# Convert text to lowercase using the `tr` shell command\n","processed_rdd = rdd.pipe(\"tr 'A-Z' 'a-z'\")\n","\n","print(processed_rdd.collect())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"14995f92-c7e5-424b-b874-0ae4d6ab2197","normalized_state":"finished","queued_time":"2025-02-07T19:44:45.3255357Z","session_start_time":null,"execution_start_time":"2025-02-07T19:44:45.471771Z","execution_finish_time":"2025-02-07T19:44:46.2725454Z","parent_msg_id":"d86475ba-c550-4488-a41b-2a1e032c0d67"},"text/plain":"StatementMeta(, 14995f92-c7e5-424b-b874-0ae4d6ab2197, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['hello world', 'pyspark is awesome', 'piping in spark']\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"32ecb1cf-5973-4590-baeb-db2590b10893"},{"cell_type":"markdown","source":["**Key Considerations When Using pipe()**\n","\n","1. Each partition runs separately:\n","\n","- Spark pipes data per partition, meaning the external process receives only part of the dataset at a time.\n","\n","2. Processes run in parallel:\n","\n","- If Spark has multiple partitions, multiple instances of the external script will run in parallel.\n","\n","3. Performance Overhead:\n","\n","- Using pipe() adds overhead because data needs to be serialized and transferred to external processes.\n","4. Data Format Compatibility:\n","\n","- Ensure your external script reads stdin and writes stdout correctly to avoid errors."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e5a87bc-95b7-44cd-99d7-09fd98d47e7c"},{"cell_type":"markdown","source":["# 09. Does Apache Spark support checkpointing? How and why is it used?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e97cba4a-8f7d-4efe-be57-dc50fdc89bd4"},{"cell_type":"markdown","source":["- Yes, Apache Spark supports checkpointing, which is a mechanism to persist RDDs or streaming state to a reliable storage location (HDFS, Azure Data Lake, or Microsoft Fabric Lakehouse).\n","\n","- Checkpointing is primarily used for fault tolerance and long-running applications, ensuring that Spark does not recompute lost data from scratch.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e8dcda11-fc75-46d1-9bf2-ce0315990ddb"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Initialize Spark Session\n","#spark = SparkSession.builder.appName(\"CheckpointExample\").getOrCreate()\n","\n","# Set checkpoint directory (should be HDFS, ADLS, or OneLake in Fabric)\n","spark.sparkContext.setCheckpointDir(\"abfss://rrws1@onelake.dfs.fabric.microsoft.com/rrLakeHouse.Lakehouse/Files\")\n","\n","# Create an RDD\n","rdd = spark.sparkContext.parallelize(range(1, 1000000))\n","\n","# Apply transformations\n","rdd_transformed = rdd.map(lambda x: x * 2)\n","\n","# Apply checkpoint\n","rdd_transformed.checkpoint()\n","\n","# Trigger computation\n","print(rdd_transformed.count())  # Forces checkpointing\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"784addaa-d8bc-47cf-9497-ddaeb564042f","normalized_state":"finished","queued_time":"2025-02-07T20:15:45.6752311Z","session_start_time":"2025-02-07T20:15:45.6767062Z","execution_start_time":"2025-02-07T20:16:06.2173215Z","execution_finish_time":"2025-02-07T20:16:11.5364463Z","parent_msg_id":"619ad205-de56-430c-a7b1-6f03a94b0897"},"text/plain":"StatementMeta(, 784addaa-d8bc-47cf-9497-ddaeb564042f, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["999999\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78da0bf4-97b1-46ac-8c6f-882dca1be714"},{"cell_type":"markdown","source":["# 10. What is the Spark Driver, and what role does it play in a PySpark job?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10ece052-2867-4a8d-94b6-c61e03143962"},{"cell_type":"markdown","source":["- The Spark Driver is the \"brain\" of a Spark job, managing execution and scheduling tasks.\n","- It runs in different locations based on the deployment mode (local, client, cluster).\n","- In PySpark, all transformations and actions are initiated by the driver.\n","- Too many operations on the driver (like collect()) can cause memory overload."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a9bb6e9c-4032-4b4f-9841-5efe4a8efc6b"},{"cell_type":"code","source":["# View current Spark configurations (including driver settings)\n","for key, value in spark.sparkContext.getConf().getAll():\n","    if \"driver\" in key:\n","        print(f\"{key}: {value}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"a99c59a5-4862-4dae-b4a8-67d84a7c367f","normalized_state":"finished","queued_time":"2025-02-07T20:40:07.7813357Z","session_start_time":"2025-02-07T20:40:07.7828761Z","execution_start_time":"2025-02-07T20:40:29.1897539Z","execution_finish_time":"2025-02-07T20:40:32.3793904Z","parent_msg_id":"43afb7b1-9689-473e-b301-09a3cbb8828f"},"text/plain":"StatementMeta(, a99c59a5-4862-4dae-b4a8-67d84a7c367f, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["spark.driver.cores: 8\nspark.tracking.driverLogUrl: https://sparkui.fabric.microsoft.com/sparkui/ce42e2da-274a-411e-b5cc-3ee363a1f9f5/api/v1/wt/datacloud/workspaces/98012181-c620-4a02-9dbc-41f7bae43668/activities/a99c59a5-4862-4dae-b4a8-67d84a7c367f/applications/application_1738960707004_0001/driverlog/stderr?capacityId=1db03493-a167-438f-826c-3d5e684c58e2&pbiApi=api.fabric.microsoft.com&artifactId=44e61aca-83ff-4b17-a6a7-1f70a4f6556e\nspark.driver.extraClassPath: /usr/lib/library-manager/bin/libraries/scala/*:/usr/lib/dw-connector/fabric/*\nspark.driver.host: vm-60b73904\nspark.driver.memory: 56g\nspark.driver.extraLibraryPath: /usr/hdp/current/hadoop-client/lib/native:/opt/gluten/dep\nspark.driver.maxResultSize: 4096m\nspark.driver.memoryOverhead: 384\nspark.driver.port: 36001\nspark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -Dlog4j2.configurationFile=file:/usr/hdp/current/spark3-client/conf/driver-log4j2.properties -Detwlogger.component=sparkdriver -DlogFilter.filename=SparkLogFilters.xml -DpatternGroup.filename=SparkPatternGroups.xml -Dlog4jspark.root.logger=INFO,console,RFA,ETW,Anonymizer -Dlog4jspark.log.dir=/var/log/sparkapp/${user.name} -Dlog4jspark.log.file=sparkdriver.log -Djavax.xml.parsers.SAXParserFactory=com.sun.org.apache.xerces.internal.jaxp.SAXParserFactoryImpl -XX:+UseParallelGC -XX:+UseParallelOldGC -XX:+TieredCompilation -XX:Tier4CompileThreshold=150000 -XX:OnError=\"sed '/siginfo/q;p' hs_err_pid%p.log 1>&2\" -noverify\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"25f0dcf3-3a3a-49c6-85ce-166ac7af741c"},{"cell_type":"markdown","source":["# 11. What is SparkSession, and why is it essential?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb945c5a-d615-4ee0-b573-e5f06fe92797"},{"cell_type":"markdown","source":["- SparkSession is the entry point for working with Apache Spark in PySpark. It provides a unified API for working with DataFrames, Datasets, SQL, and RDDs.\n","\n","- In Spark 2.0+, SparkSession replaces SparkContext, SQLContext, and HiveContext, combining them into a single object.\n","- SparkSession is the main entry point for using Spark\n","- It manages data processing, SQL queries, and Spark jobs\n","- Microsoft Fabric provides a default SparkSession (spark)\n","- Use .config() to optimize performance (e.g., spark.sql.shuffle.partitions)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a22803aa-931e-4ab4-abff-9b6e5f94e387"},{"cell_type":"code","source":["type(spark)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"a99c59a5-4862-4dae-b4a8-67d84a7c367f","normalized_state":"finished","queued_time":"2025-02-07T20:44:24.9392801Z","session_start_time":null,"execution_start_time":"2025-02-07T20:44:25.1421565Z","execution_finish_time":"2025-02-07T20:44:25.4914791Z","parent_msg_id":"92c8ad6a-d36a-4af2-b9fe-de23265f8cf6"},"text/plain":"StatementMeta(, a99c59a5-4862-4dae-b4a8-67d84a7c367f, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"pyspark.sql.session.SparkSession"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ca873d9e-b2f5-4824-88ee-329d26c8d114"},{"cell_type":"code","source":["print(spark)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"a99c59a5-4862-4dae-b4a8-67d84a7c367f","normalized_state":"finished","queued_time":"2025-02-07T20:46:03.5389921Z","session_start_time":null,"execution_start_time":"2025-02-07T20:46:03.7469339Z","execution_finish_time":"2025-02-07T20:46:04.0071922Z","parent_msg_id":"579d2619-9ab3-4ce5-8aed-384375e6b9bc"},"text/plain":"StatementMeta(, a99c59a5-4862-4dae-b4a8-67d84a7c367f, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7741904e2bd0>\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a6364d6-7606-420c-8f15-008e690a0299"},{"cell_type":"code","source":["print(spark.conf.get(\"spark.app.name\"))\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"a99c59a5-4862-4dae-b4a8-67d84a7c367f","normalized_state":"finished","queued_time":"2025-02-07T20:46:22.8431545Z","session_start_time":null,"execution_start_time":"2025-02-07T20:46:22.9830981Z","execution_finish_time":"2025-02-07T20:46:23.2593611Z","parent_msg_id":"2debf420-b8e5-4fee-93f2-4a032e04e104"},"text/plain":"StatementMeta(, a99c59a5-4862-4dae-b4a8-67d84a7c367f, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["SynapseEnvPy\n"]}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fda253f-a6c0-4250-a912-35952d7fde05"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder \\\n","    .appName(\"MyCustomFabricJob\") \\\n","    .config(\"spark.sql.shuffle.partitions\", \"50\") \\\n","    .getOrCreate()\n","\n","print(spark.conf.get(\"spark.app.name\"))  # Output: MyCustomFabricJob\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"a99c59a5-4862-4dae-b4a8-67d84a7c367f","normalized_state":"finished","queued_time":"2025-02-07T20:47:17.1730005Z","session_start_time":null,"execution_start_time":"2025-02-07T20:47:17.353315Z","execution_finish_time":"2025-02-07T20:47:17.6224311Z","parent_msg_id":"c4675bdd-c472-442c-9d17-4e8057ae42b8"},"text/plain":"StatementMeta(, a99c59a5-4862-4dae-b4a8-67d84a7c367f, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["MyCustomFabricJob\n"]}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8c732adc-1d6a-43f3-9d20-a67bceca0b37"},{"cell_type":"markdown","source":["# 12. What are Broadcast Variables, and how do they optimize PySpark jobs?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee9fe448-40e2-41ea-8a71-2c177e302cd1"},{"cell_type":"markdown","source":["âœ” Broadcast variables enable efficient sharing of small, read-only data across all worker nodes.\n","\n","âœ” They reduce data transfer overhead and optimize performance by minimizing network communication.\n","\n","âœ” Best for joining small reference tables with large datasets or using small datasets across multiple operations.\n","\n","âœ” Use sparingly, as broadcasting large datasets can lead to memory overhead."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7fbcc292-1d3d-48e5-ac8b-2f0dc14bc1be"},{"cell_type":"markdown","source":["# 13. How do you handle data skewness in PySpark?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9296cec7-cc50-4007-9345-95627a61edc2"},{"cell_type":"markdown","source":["|Technique\t|Use Case\t|Benefit |\n","| --- | ---|  --- |\n","Salting Keys |\tSkewed joins with large key imbalance\t|Distributes skewed data more evenly\n","Repartitioning\t|Before a skewed join or aggregation\t|Reduces skewed partitions before shuffle\n","Broadcasting\t|Small lookup table in joins\t|Avoids shuffling large table, reduces data transfer\n","skewedJoin Optimization\t| Join with skewed keys\t| Optimizes skewed joins, handles large partitions separately\n","Coalesce Partitions\t| After large shuffles\t| Reduces task overhead by reducing partition count\n","mapPartitions()\t| Custom transformations\t|Allows fine-grained control over partition processing"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f132d15b-662c-4124-8d30-d888e649b2d2"},{"cell_type":"markdown","source":["# 14 . Whatâ€™s the difference between coalesce() and repartition()?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35c916ec-c243-4321-a0a2-b4fcd750f5a3"},{"cell_type":"markdown","source":["| Feature |\trepartition() |\tcoalesce() |\n","| --- | --- | --- |\n","Purpose\t| Increase or decrease partitions\t| Decrease partitions\n","Shuffle\t| Full shuffle across partitions\t| No full shuffle, just merging\n","When to Use\t| When increasing partitions or need random distribution\t| When reducing partitions (after filtering or transformation)\n","Performance\t| Slower (because of full shuffle)\t| Faster (no full shuffle)\n","Use Case Example\t| Before large joins, aggregations\t| Before writing data to disk (to reduce number of output files)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"305c54e0-7393-47f6-9d86-e0f27b9a538a"},{"cell_type":"markdown","source":["# 15. Explain Lazy Evaluation in PySpark with an example."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"06ec50cc-3b0a-4178-8c01-e26eb0eee909"},{"cell_type":"markdown","source":["- Transformations: Operations like filter(), map(), select(), groupBy(), join() are transformations in Spark. These transformations are not executed immediately; instead, they are recorded as a logical plan.\n","\n","- Actions: Operations like collect(), count(), show(), first(), save() are actions. These trigger the actual execution of the transformations and the computation."],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"58fe93b0-5336-4720-b801-9e70982b15cf"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a Spark session\n","# spark = SparkSession.builder.master(\"local\").appName(\"Lazy Evaluation Example\").getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [\n","    (\"Alice\", 34),\n","    (\"Bob\", 45),\n","    (\"Charlie\", 23),\n","    (\"David\", 40),\n","    (\"Eve\", 29)\n","]\n","\n","columns = [\"name\", \"age\"]\n","\n","df = spark.createDataFrame(data, columns)\n","\n","# Perform a series of transformations\n","df_filtered = df.filter(df.age > 30)  # Filter out rows where age <= 30\n","df_filtered = df_filtered.withColumn(\"age2\",df_filtered.age+2)  # add one column\n","df_selected = df_filtered.select(\"name\",\"age2\")  # Select only the 'name' column\n","\n","\n","# At this point, Spark hasn't executed the operations yet.\n","\n","# Perform an action to trigger the computation\n","df_selected.show()  # This will trigger the execution of all previous transformations\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"01d67bce-b104-47cd-b1e2-f670eefc0117","normalized_state":"finished","queued_time":"2025-02-07T22:33:25.2844491Z","session_start_time":"2025-02-07T22:33:25.2858259Z","execution_start_time":"2025-02-07T22:33:44.1569834Z","execution_finish_time":"2025-02-07T22:33:47.5495936Z","parent_msg_id":"9a52f64f-4f01-4e6c-8515-0ba73aacfd14"},"text/plain":"StatementMeta(, 01d67bce-b104-47cd-b1e2-f670eefc0117, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["+-----+----+\n| name|age2|\n+-----+----+\n|Alice|  36|\n|  Bob|  47|\n|David|  42|\n+-----+----+\n\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c30c8612-ce09-4723-a871-d9ed8327b302"},{"cell_type":"code","source":["df_selected.explain()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"01d67bce-b104-47cd-b1e2-f670eefc0117","normalized_state":"finished","queued_time":"2025-02-07T22:33:37.5128008Z","session_start_time":null,"execution_start_time":"2025-02-07T22:33:47.7361251Z","execution_finish_time":"2025-02-07T22:33:47.9876577Z","parent_msg_id":"bed32e53-ce9c-4dfb-bc83-93936adcf045"},"text/plain":"StatementMeta(, 01d67bce-b104-47cd-b1e2-f670eefc0117, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["== Physical Plan ==\n*(1) Project [name#694, (age#695L + 2) AS age2#698L]\n+- *(1) Filter (isnotnull(age#695L) AND (age#695L > 30))\n   +- *(1) Scan ExistingRDD[name#694,age#695L]\n\n\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b9a8b53e-5a3d-446c-a4e7-1b22433469f8"},{"cell_type":"code","source":["rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5, 6, 7, 8])\n","\n","# Lazy evaluation example\n","result = rdd.filter(lambda x: x % 2 == 0).map(lambda x: x * 2)  # Filter even numbers and then double them\n","\n","# No computation yet, since this is lazy evaluation\n","\n","# Trigger the computation\n","result.collect()  # Now Spark will apply both the filter and map operations in a single step\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"01d67bce-b104-47cd-b1e2-f670eefc0117","normalized_state":"finished","queued_time":"2025-02-07T22:38:08.1488072Z","session_start_time":null,"execution_start_time":"2025-02-07T22:38:08.3163134Z","execution_finish_time":"2025-02-07T22:38:20.545666Z","parent_msg_id":"2bed7449-bdba-4caa-b64b-1149734c05d1"},"text/plain":"StatementMeta(, 01d67bce-b104-47cd-b1e2-f670eefc0117, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"[4, 8, 12, 16]"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0235e39-a6f5-4d40-9173-16db0a1ca36f"},{"cell_type":"markdown","source":["# 16. Key Differences Between Narrow and Wide Transformations\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"3ce1955e-5f1c-4982-a65d-8eeefb36e96f"},{"cell_type":"markdown","source":["| Aspect\t| Narrow Transformations\t| Wide Transformations\n","| --- | --- | --- |\n","Shuffling\t| No shuffling of data. Data stays within the same partition.\t| Data is shuffled across partitions or nodes.\n","Performance\t| Faster, as there is no need for data movement.\t| Slower, due to the cost of shuffling and network I/O.\n","Examples\t| map(), filter(), select(), flatMap(), sample()\t| groupByKey(), reduceByKey(), join(), distinct(), repartition()\n","Data Movement\t| Data is processed locally on each partition.\t| Data is moved between partitions or nodes in the cluster.\n","Use Case\t| When operations can be performed independently on each partition.\t| When operations require combining data from multiple partitions."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd1dd144-8eab-415d-b5c8-d992b338791f"},{"cell_type":"markdown","source":["**Narrow Transformation**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d0b24cdf-8315-4b8e-a958-8602099a83f4"},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","# Create a Spark session\n","#spark = SparkSession.builder.master(\"local\").appName(\"Narrow Transformation\").getOrCreate()\n","\n","# Create a sample DataFrame\n","data = [(\"Alice\", 34), (\"Bob\", 45), (\"Charlie\", 23)]\n","columns = [\"name\", \"age\"]\n","df = spark.createDataFrame(data, columns)\n","\n","# Apply a narrow transformation - 'map' on DataFrame (using 'rdd')\n","rdd = df.rdd.map(lambda x: (x[0], x[1] * 2))  # Multiply age by 2\n","\n","# Show the result\n","print(rdd.collect())\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"3eaaeb69-cd44-443c-8832-26ddd67dd8ae","normalized_state":"finished","queued_time":"2025-02-07T23:01:30.5065671Z","session_start_time":"2025-02-07T23:01:30.5080626Z","execution_start_time":"2025-02-07T23:01:45.0424668Z","execution_finish_time":"2025-02-07T23:01:48.7855429Z","parent_msg_id":"cd2dc5aa-2f77-4755-82b2-27f06f330754"},"text/plain":"StatementMeta(, 3eaaeb69-cd44-443c-8832-26ddd67dd8ae, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[('Alice', 68), ('Bob', 90), ('Charlie', 46)]\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e10696cc-2af1-4748-a233-4e3ffeb3c871"},{"cell_type":"markdown","source":["**Wide Transformation Example (groupBy):**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc7aea91-21eb-4a2e-95f6-0dcfe97065ad"},{"cell_type":"code","source":["# Create a sample RDD with key-value pairs\n","rdd = spark.sparkContext.parallelize([(\"A\", 1), (\"B\", 2), (\"A\", 3), (\"B\", 4), (\"A\", 5)])\n","\n","# Apply a wide transformation - 'groupByKey' (grouping by keys)\n","grouped_rdd = rdd.groupByKey()\n","\n","# Show the result\n","print(grouped_rdd.collect())\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"4e32005c-daea-4ae9-ab60-36f306387aae","normalized_state":"finished","queued_time":"2025-02-08T00:31:35.4050598Z","session_start_time":"2025-02-08T00:31:35.4065097Z","execution_start_time":"2025-02-08T00:31:51.4185987Z","execution_finish_time":"2025-02-08T00:31:55.1394384Z","parent_msg_id":"19173a4d-6749-4dae-be14-fa6bfc905ade"},"text/plain":"StatementMeta(, 4e32005c-daea-4ae9-ab60-36f306387aae, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[('B', <pyspark.resultiterable.ResultIterable object at 0x767475adc410>), ('A', <pyspark.resultiterable.ResultIterable object at 0x767475ade990>)]\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db5ef94c-48c1-46e7-ac72-e3bfa4b10e5c"},{"cell_type":"markdown","source":["# 17. How do you efficiently process huge datasets in PySpark?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c79d823-be84-46c9-936c-b2b0f5565625"},{"cell_type":"markdown","source":["To efficiently process huge datasets in PySpark, focus on: \n","\n","âœ… Using optimized file formats (Parquet, ORC)\n","\n","âœ… Partitioning and bucketing wisely\n","\n","âœ… Avoiding collect() and using broadcast joins\n","\n","âœ… Minimizing shuffle operations\n","\n","``` python\n","# Optimize executors, memory, and partitions based on the dataset\n","spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n","spark.conf.set(\"spark.executor.memory\", \"8g\")\n","spark.conf.set(\"spark.driver.memory\", \"4g\")\n","\n","```\n","\n","âœ… Configuring Spark memory and AQE(Adaptive Query Execution)\n","\n","``` python\n","# Enables Spark to optimize queries dynamically\n","\n","spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n","```\n","\n","âœ… Using Pandas UDFs for better performance"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2646eaa1-812e-48d7-bbae-74e42e08645b"},{"cell_type":"markdown","source":["# 18. How does Garbage Collection work in PySpark, and how can you optimize it?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7ea56f27-8a7e-4721-b24b-7cfca07a736c"},{"cell_type":"markdown","source":["| Level\t| Optimization |\n","| --- | --- |\n","| JVM GC\t| Use G1GC, tune memory allocation, reduce shuffling, enable GC logging\n","PySpark GC\t|Use gc.collect(), optimize Pandas UDFs, free up memory, avoid large Python objects"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4ed7c18c-7fe5-443d-a6a5-56a752478b27"},{"cell_type":"markdown","source":["# 19.When Should You Use Accumulators?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3dbec5e0-6368-485e-b033-4f683be21299"},{"cell_type":"markdown","source":["\n","\n","**Accumulators are best used for:**\n","\n","- Counting Events in a Distributed Job (e.g., tracking bad records)\n","- Logging Debugging Information (e.g., counting null values in columns)\n","- Performance Metrics (e.g., counting total processed records)\n","\n","**When NOT to Use Accumulators:**\n","\n","- Not for Returning Results to Tasks: Since worker nodes cannot read accumulators, they should not be used for passing values between tasks.\n","- Not for Complex Data Structures: Use accumulators mainly for numeric values or simple data aggregations."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"398c1dc2-e521-4951-87a2-3f16e418dcf8"},{"cell_type":"markdown","source":["# 20. What is Dynamic Resource Allocation, and how does it optimize Spark performance?"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"74d8d7df-5d19-470f-abb3-6d7b7bd0d824"},{"cell_type":"markdown","source":["**1. What is Dynamic Resource Allocation?**\n","\n","- Dynamic Resource Allocation (DRA) in Spark allows an application to automatically adjust the number of executors based on workload demand. This feature helps in efficient resource utilization by scaling resources up when more tasks are running and down when executors are idle.\n","\n","**2. How Does Dynamic Resource Allocation Work?**\n","\n","- Spark starts with a minimal number of executors.\n","- If the job needs more resources, Spark requests additional executors from the cluster manager (YARN, Kubernetes, Mesos, or Standalone).\n","- If executors remain idle for a set duration, Spark automatically removes them to free resources.\n","- This process reduces resource waste and improves overall cluster efficiency.\n","\n","**3. Why Use Dynamic Resource Allocation?**\n","Benefit\t| Explanation\n","--- | ---\n","Optimized Resource Utilization\t| Executors are allocated only when needed, reducing cluster waste.\n","Cost Savings\t| Ideal for cloud environments (AWS EMR, Databricks) where billing is based on resource usage.\n","Improved Performance\t| Ensures large workloads get additional executors dynamically, preventing bottlenecks.\n","Better Cluster Sharing\t| Helps multiple Spark jobs coexist in a shared cluster by releasing unused resources.\n","\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9dc0d9e9-35c9-4c8f-bf0c-e3188200b989"},{"cell_type":"markdown","source":["**4. How to Enable Dynamic Resource Allocation?**\n","- Enable it by setting the following configuration parameters in spark-submit, spark-defaults.conf, or within a Spark application.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d9b6436f-900c-488c-9287-7ce0243ab2d7"},{"cell_type":"markdown","source":["``` py\n","--conf spark.dynamicAllocation.enabled=true\n","--conf spark.dynamicAllocation.minExecutors=2\n","--conf spark.dynamicAllocation.maxExecutors=10\n","--conf spark.dynamicAllocation.initialExecutors=2\n","--conf spark.dynamicAllocation.executorIdleTimeout=60s\n","```\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fd98113a-d501-43b9-a9c6-140c0af6f760"},{"cell_type":"markdown","source":["**5. When Should You Use Dynamic Resource Allocation?**\n","\n","âœ… Best For:\n","\n","- ETL Pipelines & Batch Jobs â†’ Jobs with variable workloads.\n","- Shared Clusters â†’ Efficient use of resources across multiple teams.\n","- Cloud Environments â†’ Saves costs by automatically scaling resources.\n","\n","âŒ Not Ideal For:\n","\n","- Streaming Jobs (Structured Streaming/Kafka) â†’ Executors should remain stable.\n","- Small Jobs â†’ Overhead of acquiring/releasing resources may slow execution.\n","- Highly Predictable Workloads â†’ Manually setting --num-executors can be more efficient"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c90a0e82-e63a-4efe-b418-cdb372bbac55"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba022f82-bb58-43fb-b330-05da69c96ae5"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"c1759277-85aa-4c03-9c99-6206e771eca9":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"Alice","2":"25"},{"0":"2","1":"Alice","2":"30"},{"0":"1","1":"Alice","2":"25"},{"0":"3","1":"Charlie","2":"35"}],"schema":[{"key":"0","name":"ID","type":"bigint"},{"key":"1","name":"Name","type":"string"},{"key":"2","name":"Age","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}},"768aa14f-e6f6-477b-bbce-1045350bb0eb":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"1","1":"Alice","2":"25"},{"0":"2","1":"Alice","2":"30"},{"0":"3","1":"Charlie","2":"35"}],"schema":[{"key":"0","name":"ID","type":"bigint"},{"key":"1","name":"Name","type":"string"},{"key":"2","name":"Age","type":"bigint"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df_no_duplicates"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["1"],"seriesFieldKeys":["1"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"46877ff4-4111-4b04-8a9b-7e56752554a9","known_lakehouses":[{"id":"46877ff4-4111-4b04-8a9b-7e56752554a9"}],"default_lakehouse_name":"rrLakeHouse","default_lakehouse_workspace_id":"98012181-c620-4a02-9dbc-41f7bae43668"}}},"nbformat":4,"nbformat_minor":5}